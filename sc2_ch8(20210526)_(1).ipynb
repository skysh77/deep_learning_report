{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sc2_ch8(20210526) (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skysh77/deep_learning_report3/blob/main/sc2_ch8(20210526)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsqrTOxWI6uO"
      },
      "source": [
        "### H.S Choi\n",
        "+ Dept. of Urban Big Data Convergebce, University of Seoul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQVgKm_wAEq"
      },
      "source": [
        "## 8.1 어텐션의 구조\n",
        "\n",
        "+ seq2seq의 문제점과 개선 \n",
        "  + seq2seq의 입력문장의 길이에 관계없는 고정길이 $\\bf {}^eh$\n",
        "  + <그림 8-2>에서 hs는 $T \\times {\\rm embedding ~dimension}$\n",
        "  + hs 행렬의 마지막 행(마지막 입력시점 $T$)의 embedding vector\n",
        "  \n",
        "<a href=\"https://imgur.com/Je7uIDn\"><img src=\"https://i.imgur.com/Je7uIDn.png\" width=\"40%\" /></a>\n",
        "<a href=\"https://imgur.com/iUsjts4\"><img src=\"https://i.imgur.com/iUsjts4.png\" width=\"40%\" /></a>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ4amAOZGGmh"
      },
      "source": [
        "### 8.1.3 Decoder의 개선 1 \n",
        "\n",
        "<a href=\"https://imgur.com/dla59cn\"><img src=\"https://i.imgur.com/dla59cn.png\" width=\"54%\" /></a>\n",
        "<a href=\"https://imgur.com/flNmE57\"><img src=\"https://i.imgur.com/flNmE57.png\" width=\"32%\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3m8eariwA6z"
      },
      "source": [
        "> 맥락벡터(context vector)  \n",
        "\n",
        "+ $\\bf {}^eh$s: $T \\times H$ matrix \n",
        "+ $\\underbrace{\\bf a}_{1 \\times T} \\times \\underbrace{\\bf {}^eh}_{T \\times H} = \\underbrace{\\bf c}_{H \\times 1}$벡터를 맥락벡터라 함 \n",
        "+ 이 때, $T \\times 1$벡터 $\\bf a$는? \n",
        "\n",
        "<a href=\"https://imgur.com/bkEzSqr\"><img src=\"https://i.imgur.com/bkEzSqr.png\" width=\"50%\" /></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjAy1GKZNZt9"
      },
      "source": [
        "+ $\\bf a$: $T \\times 1$ 가중치 벡터  \n",
        "\n",
        "<a href=\"https://imgur.com/YiuDvuk\"><img src=\"https://i.imgur.com/YiuDvuk.png\" width=\"51%\" /></a> \n",
        "<a href=\"https://imgur.com/gDdsAqj\"><img src=\"https://i.imgur.com/gDdsAqj.png\" width=\"42%\" /></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkRpyUqAGAQ7",
        "outputId": "9143d9b6-11e6-43e4-a515-04e59f354218"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzRUUikRaPXP",
        "outputId": "7dcc1787-ead0-4045-9891-d821a1ca31c4"
      },
      "source": [
        "cd \"/content/drive/My Drive/deep-learning-from-scratch-2-master\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deep-learning-from-scratch-2-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JNSUzZVJ6gI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0f0a84-2967-4322-f77d-9f660482c654"
      },
      "source": [
        "# 맥락벡터 \n",
        "import numpy as np\n",
        "\n",
        "T, H = 5, 4\n",
        "hs = np.random.randn(T, H)\n",
        "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
        "\n",
        "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
        "print(ar.shape)\n",
        "#(5, 4)\n",
        "\n",
        "t = hs * ar\n",
        "print(t.shape)\n",
        "#(5, 4)\n",
        "\n",
        "c = np.sum(t, axis=0)\n",
        "print(c.shape)\n",
        "#(4, )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)\n",
            "(5, 4)\n",
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJrvBiEqJ356"
      },
      "source": [
        "$\\bf a$\n",
        "\n",
        "$\\bf h$s\n",
        "\n",
        "<a href=\"https://imgur.com/5TPvR3Z\"><img src=\"https://i.imgur.com/5TPvR3Z.png\" width=\"40%\" /></a>\n",
        "<a href=\"https://imgur.com/1olmA1G\"><img src=\"https://i.imgur.com/1olmA1G.png\" width=\"40%\" /></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0i4ika3TF3c"
      },
      "source": [
        "### 미니배치용\n",
        "N=10일 때, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2XqP20vKG_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e909d88e-f23a-43ef-91ad-072126f3de26"
      },
      "source": [
        "# p345\n",
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "a = np.random.randn(N, T)\n",
        "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
        "# ar = a.reshape(N, T, 1)\n",
        "\n",
        "t = hs * ar\n",
        "print(t.shape)\n",
        "\n",
        "c = np.sum(t, axis=1)\n",
        "print(c.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5, 4)\n",
            "(10, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwXsxgfHxK-i"
      },
      "source": [
        "<a href=\"https://imgur.com/M66OKTc\"><img src=\"https://i.imgur.com/M66OKTc.png\" width=\"20%\" /></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzb5tDfnMxh4"
      },
      "source": [
        "#p.346\n",
        "class WeightSum:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, hs, a):\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        ar = a.reshape(N, T, 1)#.repeat(T, axis=1)\n",
        "        t = hs * ar\n",
        "        c = np.sum(t, axis=1)\n",
        "\n",
        "        self.cache = (hs, ar)\n",
        "        return c\n",
        "\n",
        "    def backward(self, dc):\n",
        "        hs, ar = self.cache\n",
        "        N, T, H = hs.shape\n",
        "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
        "        dar = dt * hs\n",
        "        dhs = dt * ar\n",
        "        da = np.sum(dar, axis=2)\n",
        "\n",
        "        return dhs, da\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op28hNZ8MmKI"
      },
      "source": [
        "### 8.1.4 Decoder 개선 2\n",
        "\n",
        "+ Decoder의 특정시점의 은닉벡터를 $\\bf h^d$라 하자.\n",
        "+ ${\\bf h^d}$: $H \\times 1$ 벡터 \n",
        "+ encoder $\\bf {}^eh$s: $T \\times H$\n",
        "+ $\\bf {}^eh$s $\\times {\\bf h^d}$\n",
        "+ $\\underbrace{{\\bf {}^eh}{s}}_{T \\times H} \\times \\underbrace{{\\bf h^d}}_{H \\times 1} = \\underbrace{\\bf s}_{T \\times 1}$ 스코어 벡터\n",
        "+ ${\\bf a}={\\rm softmax}({\\bf s})$\n",
        "\n",
        "<a href=\"https://imgur.com/GrrtOG5\"><img src=\"https://i.imgur.com/GrrtOG5.png\" width=\"49%\" /></a>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulv45fLVGvVK"
      },
      "source": [
        "<a href=\"https://imgur.com/lvRJYoN\"><img src=\"https://i.imgur.com/lvRJYoN.png\" width=\"50%\" /></a>\n",
        "<a href=\"https://imgur.com/7jQ1QBF\"><img src=\"https://i.imgur.com/7jQ1QBF.png\" width=\"44%\" /></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G7lYfEsNd8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "265b29c6-2929-4db0-ea24-db6f606e8c6a"
      },
      "source": [
        "# p.350\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.layers import Softmax\n",
        "import numpy as np\n",
        "\n",
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "h = np.random.randn(N, H)\n",
        "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
        "\n",
        "t = hs * hr\n",
        "print(t.shape)\n",
        "\n",
        "s = np.sum(t, axis=2)\n",
        "print(s.shape)\n",
        "\n",
        "softmax=Softmax()\n",
        "a = softmax.forward(s)\n",
        "print(a.shape)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5, 4)\n",
            "(10, 5)\n",
            "(10, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPtq8W66Nogg"
      },
      "source": [
        "<a href=\"https://imgur.com/YiKXdZJ\"><img src=\"https://i.imgur.com/YiKXdZJ.png\" width=\"30%\" /></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zPKFBs3NsdY"
      },
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "from common.np import *\n",
        "from common.layers import Softmax\n",
        "\n",
        "class AttentionWeight:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.softmax = Softmax()\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, hs, h):\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
        "        t = hs * hr\n",
        "        s = np.sum(t, axis=2)\n",
        "        a = self.softmax.forward(s)\n",
        "\n",
        "        self.cache = (hs, hr)\n",
        "        return a\n",
        "\n",
        "    def backward(self, da):\n",
        "        hs, hr = self.cache\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        ds = self.softmax.backward(da)\n",
        "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
        "        dhs = dt * hr\n",
        "        dhr = dt * hs\n",
        "        dh = np.sum(dhr, axis=1)\n",
        "\n",
        "        return dhs, dh\n",
        "\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1-yXt2xNupZ"
      },
      "source": [
        "### 8.1.5 Decoder 개선 3\n",
        "\n",
        "<a href=\"https://imgur.com/r1JXaCp\"><img src=\"https://i.imgur.com/r1JXaCp.png\" width=\"40%\" /></a>\n",
        "<a href=\"https://imgur.com/ZeNy6Be\"><img src=\"https://i.imgur.com/ZeNy6Be.png\" width=\"40%\" /></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SeIJdz8OBBx"
      },
      "source": [
        "<a href=\"https://imgur.com/BsnOqyz\"><img src=\"https://i.imgur.com/BsnOqyz.png\" width=\"40%\" /></a>\n",
        "<a href=\"https://imgur.com/atwEru1\"><img src=\"https://i.imgur.com/atwEru1.png\" width=\"40%\" /></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv8nLAcPN7aB"
      },
      "source": [
        "#p.354\n",
        "class Attention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.attention_weight_layer = AttentionWeight()\n",
        "        self.weight_sum_layer = WeightSum()\n",
        "        self.attention_weight = None\n",
        "\n",
        "    def forward(self, hs, h):\n",
        "        a = self.attention_weight_layer.forward(hs, h)\n",
        "        out = self.weight_sum_layer.forward(hs, a)\n",
        "        self.attention_weight = a\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
        "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
        "        dhs = dhs0 + dhs1\n",
        "        return dhs, dh\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmiiYNdHOSRw"
      },
      "source": [
        "<a href=\"https://imgur.com/4yHitBU\"><img src=\"https://i.imgur.com/4yHitBU.png\" width=\"60%\" /></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bClKdRSHOgsG"
      },
      "source": [
        "class TimeAttention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.layers = None\n",
        "        self.attention_weights = None\n",
        "\n",
        "    def forward(self, hs_enc, hs_dec):\n",
        "        N, T, H = hs_dec.shape\n",
        "        out = np.empty_like(hs_dec)\n",
        "        self.layers = []\n",
        "        self.attention_weights = []\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = Attention()\n",
        "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
        "            self.layers.append(layer)\n",
        "            self.attention_weights.append(layer.attention_weight)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, H = dout.shape\n",
        "        dhs_enc = 0\n",
        "        dhs_dec = np.empty_like(dout)\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dhs, dh = layer.backward(dout[:, t, :])\n",
        "            dhs_enc += dhs\n",
        "            dhs_dec[:,t,:] = dh\n",
        "\n",
        "        return dhs_enc, dhs_dec\n",
        "\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxTGiavOyF12"
      },
      "source": [
        "## 8.2 Attention을 갖춘 seq2seq 구현\n",
        "\n",
        "+ AttentionEncoder class\n",
        "\n",
        "<a href=\"https://imgur.com/LagRRK9\"><img src=\"https://i.imgur.com/LagRRK9.png\" width=\"40%\" /></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GpamY4tMS5b"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "from ch08.attention_layer import TimeAttention\n",
        "\n",
        "\n",
        "class AttentionEncoder(Encoder):\n",
        "    def forward(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        dout = self.lstm.backward(dhs)\n",
        "        dout = self.embed.backward(dout)\n",
        "        return dout"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbpbdS66PC1j"
      },
      "source": [
        "#p.359\n",
        "class AttentionDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.attention = TimeAttention()\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, enc_hs):\n",
        "        h = enc_hs[:,-1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        dec_hs = self.lstm.forward(out)\n",
        "        c = self.attention.forward(enc_hs, dec_hs)\n",
        "        out = np.concatenate((c, dec_hs), axis=2)\n",
        "        score = self.affine.forward(out)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)\n",
        "        N, T, H2 = dout.shape\n",
        "        H = H2 // 2\n",
        "\n",
        "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
        "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
        "        ddec_hs = ddec_hs0 + ddec_hs1\n",
        "        dout = self.lstm.backward(ddec_hs)\n",
        "        dh = self.lstm.dh\n",
        "        denc_hs[:, -1] += dh\n",
        "        self.embed.backward(dout)\n",
        "\n",
        "        return denc_hs\n",
        "\n",
        "    def generate(self, enc_hs, start_id, sample_size):\n",
        "        sampled = []\n",
        "        sample_id = start_id\n",
        "        h = enc_hs[:, -1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([sample_id]).reshape((1, 1))\n",
        "\n",
        "            out = self.embed.forward(x)\n",
        "            dec_hs = self.lstm.forward(out)\n",
        "            c = self.attention.forward(enc_hs, dec_hs)\n",
        "            out = np.concatenate((c, dec_hs), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())\n",
        "            sampled.append(sample_id)\n",
        "\n",
        "        return sampled\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PUQxIVNPMcB"
      },
      "source": [
        "#p.361\n",
        "class AttentionSeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        args = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = AttentionEncoder(*args)\n",
        "        self.decoder = AttentionDecoder(*args)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrd4QeZlPQV5"
      },
      "source": [
        "#8.3 어텐션 평가\n",
        "<a href=\"https://imgur.com/gCfTsHR\"><img src=\"https://i.imgur.com/gCfTsHR.png\" width=\"40%\" /></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brROUI3lSJSi"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "sys.path.append('../ch07')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "from ch08.attention_seq2seq import AttentionSeq2seq\n",
        "from ch07.seq2seq import Seq2seq\n",
        "from ch07.seq2seq import Seq2seq, Encoder\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0PFDzRTV9aL"
      },
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# 입력 문장 반전\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 256\n",
        "batch_size = 128\n",
        "max_epoch = 10\n",
        "max_grad = 5.0"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgpnUQCVMxUF",
        "outputId": "e3553b06-a65b-49ef-b26a-e99946363d22"
      },
      "source": [
        "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "acc_list = []\n",
        "for epoch in range(max_epoch):\n",
        "    trainer.fit(x_train, t_train, max_epoch=1,\n",
        "                batch_size=batch_size, max_grad=max_grad)\n",
        "\n",
        "    correct_num = 0\n",
        "    for i in range(len(x_test)):\n",
        "        question, correct = x_test[[i]], t_test[[i]]\n",
        "        verbose = i < 10\n",
        "        correct_num += eval_seq2seq(model, question, correct,\n",
        "                                    id_to_char, verbose, is_reverse=True)\n",
        "\n",
        "    acc = float(correct_num) / len(x_test)\n",
        "    acc_list.append(acc)\n",
        "    print('정확도 %.3f%%' % (acc * 100))\n",
        "\n",
        "\n",
        "model.save_params()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
            "| 에폭 1 |  반복 21 / 351 | 시간 11[s] | 손실 3.09\n",
            "| 에폭 1 |  반복 41 / 351 | 시간 22[s] | 손실 1.90\n",
            "| 에폭 1 |  반복 61 / 351 | 시간 34[s] | 손실 1.72\n",
            "| 에폭 1 |  반복 81 / 351 | 시간 45[s] | 손실 1.46\n",
            "| 에폭 1 |  반복 101 / 351 | 시간 56[s] | 손실 1.19\n",
            "| 에폭 1 |  반복 121 / 351 | 시간 67[s] | 손실 1.14\n",
            "| 에폭 1 |  반복 141 / 351 | 시간 78[s] | 손실 1.09\n",
            "| 에폭 1 |  반복 161 / 351 | 시간 89[s] | 손실 1.06\n",
            "| 에폭 1 |  반복 181 / 351 | 시간 101[s] | 손실 1.04\n",
            "| 에폭 1 |  반복 201 / 351 | 시간 112[s] | 손실 1.03\n",
            "| 에폭 1 |  반복 221 / 351 | 시간 123[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 241 / 351 | 시간 134[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 261 / 351 | 시간 145[s] | 손실 1.01\n",
            "| 에폭 1 |  반복 281 / 351 | 시간 156[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 301 / 351 | 시간 168[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 321 / 351 | 시간 179[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 341 / 351 | 시간 190[s] | 손실 1.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "정확도 0.000%\n",
            "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 2 |  반복 21 / 351 | 시간 11[s] | 손실 1.00\n",
            "| 에폭 2 |  반복 41 / 351 | 시간 22[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 61 / 351 | 시간 33[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 81 / 351 | 시간 45[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 101 / 351 | 시간 56[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 121 / 351 | 시간 67[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 141 / 351 | 시간 78[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 161 / 351 | 시간 89[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 181 / 351 | 시간 100[s] | 손실 0.97\n",
            "| 에폭 2 |  반복 201 / 351 | 시간 111[s] | 손실 0.95\n",
            "| 에폭 2 |  반복 221 / 351 | 시간 122[s] | 손실 0.94\n",
            "| 에폭 2 |  반복 241 / 351 | 시간 133[s] | 손실 0.90\n",
            "| 에폭 2 |  반복 261 / 351 | 시간 144[s] | 손실 0.83\n",
            "| 에폭 2 |  반복 281 / 351 | 시간 155[s] | 손실 0.74\n",
            "| 에폭 2 |  반복 301 / 351 | 시간 166[s] | 손실 0.66\n",
            "| 에폭 2 |  반복 321 / 351 | 시간 178[s] | 손실 0.58\n",
            "| 에폭 2 |  반복 341 / 351 | 시간 189[s] | 손실 0.47\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 2006-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 2007-08-09\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1983-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 2016-11-08\n",
            "---\n",
            "정확도 51.320%\n",
            "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 3 |  반복 21 / 351 | 시간 11[s] | 손실 0.30\n",
            "| 에폭 3 |  반복 41 / 351 | 시간 23[s] | 손실 0.21\n",
            "| 에폭 3 |  반복 61 / 351 | 시간 34[s] | 손실 0.14\n",
            "| 에폭 3 |  반복 81 / 351 | 시간 45[s] | 손실 0.09\n",
            "| 에폭 3 |  반복 101 / 351 | 시간 57[s] | 손실 0.07\n",
            "| 에폭 3 |  반복 121 / 351 | 시간 68[s] | 손실 0.05\n",
            "| 에폭 3 |  반복 141 / 351 | 시간 79[s] | 손실 0.04\n",
            "| 에폭 3 |  반복 161 / 351 | 시간 90[s] | 손실 0.03\n",
            "| 에폭 3 |  반복 181 / 351 | 시간 102[s] | 손실 0.03\n",
            "| 에폭 3 |  반복 201 / 351 | 시간 113[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 221 / 351 | 시간 124[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 241 / 351 | 시간 136[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 261 / 351 | 시간 147[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 281 / 351 | 시간 158[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 301 / 351 | 시간 170[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 321 / 351 | 시간 181[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 341 / 351 | 시간 193[s] | 손실 0.01\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 99.900%\n",
            "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 21 / 351 | 시간 11[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 41 / 351 | 시간 23[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 61 / 351 | 시간 34[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 81 / 351 | 시간 45[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 101 / 351 | 시간 56[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 121 / 351 | 시간 67[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 141 / 351 | 시간 79[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 161 / 351 | 시간 90[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 181 / 351 | 시간 101[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 201 / 351 | 시간 112[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 221 / 351 | 시간 124[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 241 / 351 | 시간 135[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 261 / 351 | 시간 146[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 281 / 351 | 시간 157[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 301 / 351 | 시간 168[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 321 / 351 | 시간 180[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 341 / 351 | 시간 191[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 99.900%\n",
            "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 41 / 351 | 시간 22[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 81 / 351 | 시간 45[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 101 / 351 | 시간 56[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 121 / 351 | 시간 68[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 141 / 351 | 시간 79[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 161 / 351 | 시간 90[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 181 / 351 | 시간 101[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 201 / 351 | 시간 113[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 221 / 351 | 시간 124[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 241 / 351 | 시간 135[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 261 / 351 | 시간 147[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 281 / 351 | 시간 158[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 301 / 351 | 시간 169[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 321 / 351 | 시간 181[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 341 / 351 | 시간 192[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 81 / 351 | 시간 45[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 101 / 351 | 시간 56[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 121 / 351 | 시간 68[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 141 / 351 | 시간 79[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 161 / 351 | 시간 90[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 181 / 351 | 시간 101[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 201 / 351 | 시간 113[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 221 / 351 | 시간 124[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 241 / 351 | 시간 135[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 261 / 351 | 시간 146[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 281 / 351 | 시간 158[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 301 / 351 | 시간 169[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 321 / 351 | 시간 180[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 341 / 351 | 시간 191[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 81 / 351 | 시간 45[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 101 / 351 | 시간 56[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 121 / 351 | 시간 67[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 141 / 351 | 시간 79[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 161 / 351 | 시간 90[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 181 / 351 | 시간 101[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 201 / 351 | 시간 112[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 221 / 351 | 시간 124[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 241 / 351 | 시간 135[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 261 / 351 | 시간 146[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 281 / 351 | 시간 157[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 301 / 351 | 시간 168[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 321 / 351 | 시간 180[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 341 / 351 | 시간 191[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 41 / 351 | 시간 22[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 61 / 351 | 시간 33[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 81 / 351 | 시간 44[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 101 / 351 | 시간 55[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 121 / 351 | 시간 66[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 141 / 351 | 시간 78[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 161 / 351 | 시간 89[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 181 / 351 | 시간 100[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 201 / 351 | 시간 111[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 221 / 351 | 시간 122[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 241 / 351 | 시간 133[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 261 / 351 | 시간 144[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 281 / 351 | 시간 155[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 301 / 351 | 시간 166[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 321 / 351 | 시간 177[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 341 / 351 | 시간 188[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 41 / 351 | 시간 22[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 61 / 351 | 시간 33[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 81 / 351 | 시간 44[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 101 / 351 | 시간 56[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 121 / 351 | 시간 67[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 141 / 351 | 시간 78[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 161 / 351 | 시간 89[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 181 / 351 | 시간 100[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 201 / 351 | 시간 111[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 221 / 351 | 시간 123[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 241 / 351 | 시간 134[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 261 / 351 | 시간 145[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 281 / 351 | 시간 156[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 301 / 351 | 시간 167[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 321 / 351 | 시간 178[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 341 / 351 | 시간 190[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 41 / 351 | 시간 22[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 61 / 351 | 시간 33[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 81 / 351 | 시간 45[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 101 / 351 | 시간 56[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 121 / 351 | 시간 67[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 141 / 351 | 시간 78[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 161 / 351 | 시간 89[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 181 / 351 | 시간 100[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 201 / 351 | 시간 111[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 221 / 351 | 시간 123[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 241 / 351 | 시간 134[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 261 / 351 | 시간 145[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 281 / 351 | 시간 156[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 301 / 351 | 시간 167[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 321 / 351 | 시간 178[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 341 / 351 | 시간 189[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "kWHXGn52Vzcl",
        "outputId": "0ad0d293-21f5-4eca-d941-0dd6caeb5790"
      },
      "source": [
        "# 그래프 그리기\n",
        "x = np.arange(len(acc_list))\n",
        "plt.plot(x, acc_list, marker='o')\n",
        "plt.xlabel('에폭')\n",
        "plt.ylabel('정확도')\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54253 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54869 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54253 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54869 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXSElEQVR4nO3dfXBd9Z3f8fdX8gMyGBuwAlg22M06Bi+GOFEJCZuWCWQBk9gM3XbCTNImky7/LNukydBC0tKU/pHs0tntzizdXZrmYXezoSyllhK8cZOQNE26yeLEso1tnHgdHiz5QQaMDfhR/vYPXceyLNuSraNz7z3v14zGuuceX33m2tJH93vOPb/ITCRJ1dVSdgBJUrksAkmqOItAkirOIpCkirMIJKniJpUdYKxmzZqV8+bNKzuGJDWUn/70p7szs32k+xquCObNm8fq1avLjiFJDSUiXjjVfY6GJKniLAJJqjiLQJIqziKQpIqzCCSp4go7aygivgR8ANiVmdeMcH8AfwQsBd4EPpqZPysqj45bsaaXh1dtpm/PfmbPbOO+Wxdy55IOc5SYox4ymKO6OYo8ffQrwB8Df36K+28HFtQ+3gX8Se1PFWjFml4eeHI9+w8PANC7Zz8PPLkeYEL/g5ujvjKYo9o5osjLUEfEPOCbp3hF8GfA9zPz67Xbm4GbMnP76R6zs7MzfR/B2bvxC0/Tu2f/SdsvPn8KX7hr8YTluP/J9bzyxqGTtl80bTIPLb+GBDKTTDg69M8Tto+07fifMPQ2JIOfZ+bg7YQv/nAr+w4cOSnH9KmT+NiN8wp9Do758o+eZ9/BcjOYo/FydMxs40f3v2/UjxMRP83MzhHvK7EIvgl8ITN/WLv9XeDfZuZJP+Uj4h7gHoArrrjinS+8cMr3RegM5t//FK5AMToRE/N1TvctOFEZzNF4OQL45RfuGPXjnK4IGuKdxZn5KPAoDL4iKDlOQ5s9s23EVwTt06fy5Y/+wwnL8bGvPEP/voMnbX/L9Kl87V++i4ggAloiCGp/BrWPoGXIfSPtS0DLkH2D439/6H7v/f2n6d1z4KQcY/1t61yc6lXaRGYwR+PlmD2zbdy+RplF0AvMHXJ7Tm2bCnTfrQv59F+vZeDo8T5tm9zKZ5dezTUdMyYsx2eXXn3C3PNYjs8svZoFl06fsBz33XrViDnuu3XhBGZYWHoGc1Q7R5mnj3YD/zwG3QC8dqbjAzp3SxdfzpTWoG1yK8Hgbzefv2vxhJ8JceeSDj5/12I6ZrZVPkc9ZDBHtXMUdowgIr4O3ATMAnYC/wGYDJCZf1o7ffSPgdsYPH30YyMdHxjOg8Xn5rubdvLxr67mSx/t5H1XXVp2HEkTpJRjBJl59xnuT+B3ivr6GllXTx8XTZvMexeMeDVaSRXkO4sr5I2DR/j2xp0sXXw5k1v9p5c0yJ8GFfKdTTvZf3iA5W+f+HdGSqpfFkGFrFjTy+wZ59F55UVlR5FURyyCinj59YP84Be7+eDbZ9PSMoHvhpFU9yyCilj57A4GjiZ3OhaSNIxFUBHdPb287dILuOqyiXuzlqTGYBFUwLZX3+SZ519l+ds7iIm8SIqkhmARVMA31g6+YXvZdbNLTiKpHlkEFdDV08s7rpjJ3IunlR1FUh2yCJrcczv28tyOfb53QNIpWQRNrrunj9aW4I5rLy87iqQ6ZRE0scykq6eP3/i1Wcy6YGrZcSTVKYugif3sxVfp3bOf5W/3ILGkU7MImlhXTx9TJ7Xwm79+WdlRJNUxi6BJHR44ylPrtnPLoku5YGpDrEgqqSQWQZP64ZbdvPzGIZb73gFJZ2ARNKnunj5mtE3mpoVvKTuKpDpnETSh/YcGWLVhB0sXX8aUSf4TSzo9f0o0oe9s2smbhwZYdp1vIpN0ZhZBE+rq6eOyC8/j+vkXlx1FUgOwCJrMnjcP8X9+vosPXnc5rS5AI2kULIIms3L9Dg4PpNcWkjRqFkGT6erp5R+0n8+vz76w7CiSGoRF0ET69uzn755/hTtdgEbSGFgETeSb6/rIdAEaSWNjETSRrp4+rps7k3mzzi87iqQGYhE0iS279rGhb6+XlJA0ZhZBk+ju6aMl4AMuQCNpjCyCJpCZrOjp4z1vncVbLjyv7DiSGoxF0AR6XtrDi6+86QI0ks6KRdAEunr6mDKphVuvcQEaSWNXaBFExG0RsTkitkTE/SPcf0VEfC8i1kTEuohYWmSeZnRk4CjfXLedm696CxeeN7nsOJIaUGFFEBGtwCPA7cAi4O6IWDRst38HPJ6ZS4APAf+1qDzN6m+3vszu1w86FpJ01op8RXA9sCUzt2bmIeAxYPmwfRI4di2EGUBfgXmaUldPH9OnTnIBGklnrcgi6ABeGnJ7W23bUJ8DPhwR24CVwO+O9EARcU9ErI6I1f39/UVkbUgHDg/wrWd3cNs1l3He5Nay40hqUGUfLL4b+EpmzgGWAn8RESdlysxHM7MzMzvb29snPGS9evq5Xbx+8IhXGpV0Toosgl5g7pDbc2rbhvo48DhAZv4tcB4wq8BMTaWrp5f26VN591svKTuKpAZWZBE8AyyIiPkRMYXBg8Hdw/Z5EbgZICKuZrAInP2Mwmv7D/O95/r54LWzXYBG0jkprAgy8whwL7AK2MTg2UEbIuKhiFhW2+3TwG9HxFrg68BHMzOLytRMVj27g0MDRz1bSNI5m1Tkg2fmSgYPAg/d9uCQzzcCNxaZoVl1re1l3iXTuHbOjLKjSGpwZR8s1lnYufcA/+/vX2aZC9BIGgcWQQP6xtrBBWgcC0kaDxZBA+pe28fijhm8tf2CsqNIagIWQYPZ2v8667a95qsBSePGImgw3Wv7iIAPXGsRSBofFkEDyUy6e/q4Yf4lXDbDBWgkjQ+LoIE827uXrbvfcCwkaVxZBA1kRU8vU1pbuP0a1yWWNH4sggYxcDT5xto+blrYzoxpLkAjafxYBA3iJ1tfZte+g15pVNK4swgaRFdPH+dPaeXmq12ARtL4sggawMEjA6x8dju3ugCNpAJYBA3g+5v72XfABWgkFcMiaABdPb1ccv4UbnQBGkkFsAjq3L4Dh/nOpl184NrLmdTqP5ek8edPljq3asNODh05yvIljoUkFcMiqHNdPb3MvbiNJXNnlh1FUpOyCOpY/76D/GjLbpZf5wI0kopjEdSxp9b1cdQFaCQVzCKoY11r+7j68gtZcOn0sqNIamIWQZ164eU3WPPiHl8NSCqcRVCnunv6AFh2nUUgqVgWQR3KTFb09HL9/IuZPbOt7DiSmpxFUIc2bt/L3/e7AI2kiWER1KHunj4mtQRLXYBG0gSwCOrM0aNJ99o+/vHb2rno/Cllx5FUARZBnfm7519h+2sHWOZYSNIEsQjqTFdPH9OmtPL+RZeWHUVSRVgEdeTQkaOsXL+d31x0KdOmTCo7jqSKsAjqyA9+3s9r+w+7AI2kCVVoEUTEbRGxOSK2RMT9p9jnn0XExojYEBF/VWSeete1to+Lpk3mNxbMKjuKpAopbP4QEa3AI8D7gW3AMxHRnZkbh+yzAHgAuDEzX42Iyq7M/sbBI3x74w5+651zmOwCNJImUJE/ca4HtmTm1sw8BDwGLB+2z28Dj2TmqwCZuavAPHXt2xt3cuDwUcdCkiZckUXQAbw05Pa22rah3ga8LSJ+FBE/jojbRnqgiLgnIlZHxOr+/v6C4pZrRU8vHTPbeOcVF5UdRVLFlD2DmAQsAG4C7gb+W0SctBRXZj6amZ2Z2dne3j7BEYv38usH+b+/2M2yt8+mpcUFaCRNrCKLoBeYO+T2nNq2obYB3Zl5ODN/CfycwWKolJXrtzNwNL22kKRSFFkEzwALImJ+REwBPgR0D9tnBYOvBoiIWQyOirYWmKkudfX0sfDS6Vx12YVlR5FUQYUVQWYeAe4FVgGbgMczc0NEPBQRy2q7rQJejoiNwPeA+zLz5aIy1aOXXnmT1S+86iUlJJWm0LevZuZKYOWwbQ8O+TyBT9U+Kukb61yARlK5yj5YXHlda/p455UXMffiaWVHkVRRFkGJntuxl80793mQWFKpLIISdfX00doS3LHYBWgklcciKMnRo0l3Tx/vXTCLSy6YWnYcSRVmEZTkZy++Su+e/Y6FJJXOIihJV08f501u4f2LLis7iqSKswhKcHjgKE+t384tV1/KBVNdgEZSuSyCEvzwF7t55Y1DXmlUUl0Y1a+jEfHgGXbZlZl/Og55mtqKNb08vGozvXv2EwGvvXmo7EiSNOp3Ft/A4LWCTnVpzK8CFsFprFjTywNPrmf/4QEAMuHfd21gUmsLdy7xlYGk8ox2NDSQmXsz87WRPoAsMmQzeHjV5l+VwDH7Dw/w8KrNJSWSpEGjLYIz/aC3CM6gb8/+MW2XpIky2tHQ5Ig41TWSA2gdpzxNa/bMNnpH+KE/e2ZbCWkk6bjRFsGPgU+e5v6/GYcsTe2+Wxdy3xNrOTxw/MVT2+RW7rt1YYmpJGlsp4/GaT50Bncu6eBtl06nJQafsI6ZbXz+rsUeKJZUutG+IngXnjV0TvYeOMwvdr7OR98znwc/uKjsOJL0K6MtgoHM3HuqOyPCg8Vn8N1NOzk0cJQ7rvVKo5Lqi2cNTZCn1u3g8hnnsWTuzLKjSNIJPGtoAuw9cJgf/LyfD99wJS0tHlKRVF/G46yhwLOGTsuxkKR65sHiCeBYSFI982BxwRwLSap3HiwumGMhSfXOg8UFcywkqd6N9WDxqWYb3xqfOM3FsZCkRjCqIsjM/1h0kGbkWEhSI3CpygI5FpLUCCyCghwbC91+zeWOhSTVNYugII6FJDUKi6AgjoUkNQqLoACOhSQ1kkKLICJui4jNEbElIu4/zX7/JCIyIjqLzDNRHAtJaiSFFUFEtAKPALcDi4C7I+KkFVkiYjrwCeAnRWWZaI6FJDWSIl8RXA9sycytmXkIeAxYPsJ+/wn4PeBAgVkmjGMhSY2myCLoAF4acntbbduvRMQ7gLmZ+dTpHigi7omI1RGxur+/f/yTjiPHQpIaTWkHiyOiBfgD4NNn2jczH83MzszsbG9vLz7cOXAsJKnRFFkEvcDcIbfn1LYdMx24Bvh+RDwP3AB0N/IBY8dCkhpRkUXwDLAgIuZHxBQGF7bpPnZnZr6WmbMyc15mzmPwwnbLMnN1gZkK5VhIUiMqrAgy8whwL7AK2AQ8npkbIuKhiFhW1Nctk2MhSY1otJehPiuZuRJYOWzbg6fY96YisxTt2FjoI+/2ktOSGovvLB4nx8ZCSxc7FpLUWCyCceJYSFKjsgjGwbGx0NLFni0kqfFYBOPAsZCkRmYRjAPHQpIamUVwjhwLSWp0FsE5ciwkqdFZBOfoqXXbHQtJamgWwTkYHAvtdiwkqaFZBOfAsZCkZmARnAPHQpKagUVwlhwLSWoWFsFZciwkqVlYBGfJsZCkZmERnAXHQpKaiUVwFhwLSWomFsFZcCwkqZlYBGPkWEhSs7EIxsixkKRmYxGMkWMhSc3GIhgDx0KSmpFFMAaOhSQ1I4tgDJ5at53ZjoUkNRmLYJSOjYVudywkqclYBKPkWEhSs7IIRsmxkKRmZRGMgmMhSc3MIhgFx0KSmplFMAqOhSQ1M4vgDBwLSWp2hRZBRNwWEZsjYktE3D/C/Z+KiI0RsS4ivhsRVxaZ52w4FpLU7AorgohoBR4BbgcWAXdHxKJhu60BOjPzWuAJ4PeLynO2HAtJanZFviK4HtiSmVsz8xDwGLB86A6Z+b3MfLN288fAnALzjJljIUlVUGQRdAAvDbm9rbbtVD4O/M1Id0TEPRGxOiJW9/f3j2PE03MsJKkK6uJgcUR8GOgEHh7p/sx8NDM7M7Ozvb19wnI5FpJUBUUWQS8wd8jtObVtJ4iIW4DPAssy82CBecbEsZCkqiiyCJ4BFkTE/IiYAnwI6B66Q0QsAf6MwRLYVWCWMXMsJKkqCiuCzDwC3AusAjYBj2fmhoh4KCKW1XZ7GLgA+OuI6ImI7lM83IRzLCSpKiYV+eCZuRJYOWzbg0M+v6XIr3+2jo2FPvLuKx0LSWp6dXGwuN44FpJUJRbBCBwLSaoSi2AYzxaSVDUWwTDf2ehYSFK1WATDrFzvWEhStVgEQzgWklRFFsEQjoUkVZFFMIRjIUlVZBHUOBaSVFUWQc2xsdAd1zoWklQtFkGNYyFJVWURcOJYKMKxkKRqsQhwLCSp2iwCHAtJqrbKF4FjIUlVV/kicCwkqeoqXwSOhSRVXaWLwLGQJFW8CBwLSVLFi8CxkCRVuAgcC0nSoMoWgWMhSRpU2SJwLCRJgypZBI6FJOm4ShaBYyFJOq6SReBYSJKOq1wROBaSpBNVrggcC0nSiSpXBI6FJOlElSoCx0KSdLJKFYFjIUk6WaFFEBG3RcTmiNgSEfePcP/UiPgftft/EhHzisixYk0vN37haT71+FpaA17Y/UYRX0aSGlJhRRARrcAjwO3AIuDuiFg0bLePA69m5q8Bfwj83njnWLGmlweeXE/vnv0ADCR85n89y4o1veP9pSSpIRX5iuB6YEtmbs3MQ8BjwPJh+ywHvlr7/Ang5hjn4f3Dqzaz//DACdv2Hx7g4VWbx/PLSFLDKrIIOoCXhtzeVts24j6ZeQR4Dbhk+ANFxD0RsToiVvf3948pRF/tlcBot0tS1TTEweLMfDQzOzOzs729fUx/d/bMtjFtl6SqKbIIeoG5Q27PqW0bcZ+ImATMAF4ezxD33bqQtsmtJ2xrm9zKfbcuHM8vI0kNq8gieAZYEBHzI2IK8CGge9g+3cC/qH3+W8DTmZnjGeLOJR18/q7FdMxsI4COmW18/q7F3Llk+JRKkqppUlEPnJlHIuJeYBXQCnwpMzdExEPA6szsBv478BcRsQV4hcGyGHd3LunwB78knUJhRQCQmSuBlcO2PTjk8wPAPy0ygyTp9BriYLEkqTgWgSRVnEUgSRVnEUhSxcU4n61ZuIjoB144y78+C9g9jnEanc/HiXw+jvO5OFEzPB9XZuaI78htuCI4FxGxOjM7y85RL3w+TuTzcZzPxYma/flwNCRJFWcRSFLFVa0IHi07QJ3x+TiRz8dxPhcnaurno1LHCCRJJ6vaKwJJ0jAWgSRVXGWKICJui4jNEbElIu4vO09ZImJuRHwvIjZGxIaI+ETZmepBRLRGxJqI+GbZWcoWETMj4omIeC4iNkXEu8vOVJaI+Ne175NnI+LrEXFe2ZmKUIkiiIhW4BHgdmARcHdELCo3VWmOAJ/OzEXADcDvVPi5GOoTwKayQ9SJPwK+lZlXAddR0eclIjqAfwV0ZuY1DF5Ov5BL5ZetEkUAXA9sycytmXkIeAxYXnKmUmTm9sz8We3zfQx+k1d6sYaImAPcAXyx7Cxli4gZwD9icK0QMvNQZu4pN1WpJgFttRUUpwF9JecpRFWKoAN4acjtbVT8hx9ARMwDlgA/KTdJ6f4L8G+Ao2UHqQPzgX7gy7VR2Rcj4vyyQ5UhM3uB/wy8CGwHXsvM/11uqmJUpQg0TERcAPxP4JOZubfsPGWJiA8AuzLzp2VnqROTgHcAf5KZS4A3gEoeU4uIixicHMwHZgPnR8SHy01VjKoUQS8wd8jtObVtlRQRkxksga9l5pNl5ynZjcCyiHiewZHh+yLiL8uNVKptwLbMPPYq8QkGi6GKbgF+mZn9mXkYeBJ4T8mZClGVIngGWBAR8yNiCoMHfLpLzlSKiAgG57+bMvMPys5Ttsx8IDPnZOY8Bv9fPJ2ZTflb32hk5g7gpYhYWNt0M7CxxEhlehG4ISKm1b5vbqZJD5wXumZxvcjMIxFxL7CKwSP/X8rMDSXHKsuNwEeA9RHRU9v2mdr60hLA7wJfq/3StBX4WMl5SpGZP4mIJ4CfMXi23Rqa9FITXmJCkiquKqMhSdIpWASSVHEWgSRVnEUgSRVnEUhSxVkEklRxlXgfgTTeIuJzDF699Uht0yTgxyNty8zPTXQ+aSwsAunsfejYlTkjYibwyVNsk+qaoyFJqjiLQJIqziKQpIqzCCSp4iwCSao4i0CSKs7TR6Wzswv484g4ts5xC/CtU2yT6prrEUhSxTkakqSKswgkqeIsAkmqOItAkirOIpCkivv/ebAgCHyWL88AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AtPLKzfPZiA"
      },
      "source": [
        "<a href=\"https://imgur.com/i24ycyq\"><img src=\"https://i.imgur.com/i24ycyq.png\" width=\"40%\" /></a>\n",
        "\n",
        "<a href=\"https://imgur.com/IvSzz2O\"><img src=\"https://i.imgur.com/IvSzz2O.png\" width=\"30%\" /></a> $\\qquad$\n",
        "<a href=\"https://imgur.com/0SquO8r\"><img src=\"https://i.imgur.com/0SquO8r.png\" width=\"40%\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HpSulA2Pc4Q"
      },
      "source": [
        "### 8.3.3 어텐션 시각화\n",
        "\n",
        "<a href=\"https://imgur.com/bYZhNpF\"><img src=\"https://i.imgur.com/bYZhNpF.png\" width=\"52%\" /></a>\n",
        "<a href=\"https://imgur.com/qTMMurF\"><img src=\"https://i.imgur.com/qTMMurF.png\" width=\"40%\" /></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU_ppmhaPtcB"
      },
      "source": [
        "## 8.4 어텐션 주제\n",
        "\n",
        "> 단방향 RNN vs. 양방향 RNN  \n",
        "\n",
        "<a href=\"https://imgur.com/3eoURVF\"><img src=\"https://i.imgur.com/3eoURVF.png\" width=\"40%\" /></a> $\\qquad$ \n",
        "<a href=\"https://imgur.com/SLjprLr\"><img src=\"https://i.imgur.com/SLjprLr.png\" width=\"40%\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znZA3-fKF1Pq"
      },
      "source": [
        "\n",
        "> `attention` 계층 \n",
        "\n",
        "<a href=\"https://imgur.com/Wo9WgkM\"><img src=\"https://i.imgur.com/Wo9WgkM.png\" width=\"40%\" /></a> $\\qquad$ \n",
        "<a href=\"https://imgur.com/y3dqeCm\"><img src=\"https://i.imgur.com/y3dqeCm.png\" width=\"40%\" /></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eSne8fYG-vR"
      },
      "source": [
        "> 심층화 `skip` connection  \n",
        "\n",
        "+ residual connection  \n",
        "\n",
        "\n",
        "<a href=\"https://imgur.com/viRcp5H\"><img src=\"https://i.imgur.com/viRcp5H.png\" width=\"40%\" /></a> $\\qquad$ \n",
        "<a href=\"https://imgur.com/AjtVcR7\"><img src=\"https://i.imgur.com/AjtVcR7.png\" width=\"20%\" /></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QcWBLHMP8sg"
      },
      "source": [
        "### 8.5 어텐션의 응용\n",
        "\n",
        "> Transformer (Attention is all you need)\n",
        "\n",
        "+ Key, Query, Value \n",
        "\n",
        "> BERTs (Transformer Encoder 활용)\n",
        "\n",
        "+ MLM  \n",
        "\n",
        "+ Bidirectional \n",
        "\n",
        "> GPTs (Transformer Decoder 활용)\n",
        "\n",
        "+ Unidirectional  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://imgur.com/JLHIIQR\"><img src=\"https://i.imgur.com/JLHIIQR.png\" width=\"40%\" /></a>\n",
        "\n",
        "<a href=\"https://imgur.com/B6vGlmP\"><img src=\"https://i.imgur.com/B6vGlmP.png\" width=\"40%\" /></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JNHHOEKRPFR"
      },
      "source": [
        "<a href=\"https://imgur.com/hjkR4VX\"><img src=\"https://i.imgur.com/hjkR4VX.png\" width=\"40%\" /></a>\n",
        "\n",
        "<a href=\"https://imgur.com/IArFrTJ\"><img src=\"https://i.imgur.com/IArFrTJ.png\" width=\"40%\" /></a>\n",
        "\n",
        "<a href=\"https://imgur.com/bNm3b1m\"><img src=\"https://i.imgur.com/bNm3b1m.png\" width=\"40%\" /></a>\n",
        "\n",
        "<a href=\"https://imgur.com/3j9hOnO\"><img src=\"https://i.imgur.com/3j9hOnO.png\" width=\"40%\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff3B7cOgEJW6"
      },
      "source": [
        ""
      ]
    }
  ]
}